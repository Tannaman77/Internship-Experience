{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linen \u001b[38;5;28;01mas\u001b[39;00m nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Sequence, Optional, NamedTuple\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import optax\n",
    "\n",
    "from pyscf import gto, scf\n",
    "from pyscf.dft import numint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(NamedTuple):\n",
    "    coords : Array\n",
    "    weights : Array\n",
    "\n",
    "def integrate(grid: Grid, vals = Array, axis: int = 0)-> Array:\n",
    "    return jnp.tensordot(grid.weights, vals, axes=(0, axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionalInputs(NamedTuple):\n",
    "    grid: Grid\n",
    "    rho: Array\n",
    "    grad_rho: Optional[Array] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_array(inputs: FunctionalInputs):\n",
    "    \n",
    "    _, rho, grad_rho = inputs\n",
    "\n",
    "    if grad_rho is None:\n",
    "        feature_list = [rho]\n",
    "\n",
    "    else:\n",
    "        grad_rho_norm = jnp.sum(grad_rho**2, axis=-1)\n",
    "        feature_list = [rho,grad_rho_norm]\n",
    "    return jnp.stack(feature_list, axis = -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(rho):\n",
    "    kF = (3 * jnp.pi**2)**(1/3) * rho ** (1/3)\n",
    "    return (-3 * kF)/(4 * jnp.pi)\n",
    "\n",
    "class ForwardFeedNN(nn.Module):\n",
    "    \n",
    "    layer_widths: Sequence[int]\n",
    "    out_features = 1\n",
    "    activate : Callable[[Array],Array] = jax.nn.gelu\n",
    "    squash_offset: float = 1e-4\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: FunctionalInputs):\n",
    "        \n",
    "        x = build_input_array(inputs)\n",
    "\n",
    "        h = jnp.log(jnp.abs(x)+self.squash_offset)\n",
    "        h = nn.Dense(features = self.layer_widths[0])(h)\n",
    "        h = jnp.tanh(h)\n",
    "\n",
    "        for features in self.layer_widths:\n",
    "            h = nn.Dense(features)(h)\n",
    "            h = self.activate(h)\n",
    "            h = nn.LayerNorm()(h)\n",
    "            \n",
    "        h = nn.Dense(features = self.out_features)(h)\n",
    "        return 2 * lda(inputs.rho) * jax.nn.sigmoid(h).squeeze()\n",
    "        # We have redefined the output to follow the DeepMind paper - multuply with LDA value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "\n",
    "    with h5py.File(path, 'r') as file:\n",
    "\n",
    "        coords = np.array(file['coords'])\n",
    "        weights = np.array(file['weights'])\n",
    "\n",
    "        rho = np.array(file['rho'])\n",
    "\n",
    "        grad_rho = np.array(file['grad_rho'])\n",
    "\n",
    "        exc = np.array(file['exc_pbe'])\n",
    "\n",
    "    grid = Grid(coords, weights)\n",
    "    inputs = FunctionalInputs(grid, rho, grad_rho)\n",
    "\n",
    "    return inputs, exc\n",
    "\n",
    "def load_data(folder_path):\n",
    "    paths = glob(folder_path + '/*.h5')\n",
    "    return [load_file(path) for path in paths]\n",
    "\n",
    "def separate_data(data):\n",
    "    return zip(*data)\n",
    "\n",
    "def divide_into_batches(data, batch_size):\n",
    "    return [data[i:i+batch_size] for i in range(0, len(data), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('/Users/corneliussalonis/SNSP_23/MLtut/cornelius_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) # loaded 19 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super important - train-test split\n",
    "train_data = data[:16]\n",
    "test_data = data[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batches = divide_into_batches(train_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch, test_targets = separate_data(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_exc = test_batch[0], test_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = build_input_array(test_inputs)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxc = ForwardFeedNN(layer_widths=(128,128))\n",
    "key = jax.random.PRNGKey(42)\n",
    "params = fxc.init(key, test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exc = fxc.apply(params, test_inputs)\n",
    "exc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exc.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cost_one_input(params, inputs, target_exc):\n",
    "    exc = fxc.apply(params, inputs)\n",
    "    return integrate(inputs.grid, (exc - target_exc)**2)\n",
    "\n",
    "@jax.value_and_grad\n",
    "def cost_batch(params, inputs: Sequence[FunctionalInputs], target_excs: Sequence[Array]):\n",
    "\n",
    "    batch_size = len(inputs)\n",
    "    mean_cost = 0.0\n",
    "\n",
    "    for input, target in zip(inputs, target_excs):\n",
    "        mean_cost += cost_one_input(params, input, target) / batch_size\n",
    "\n",
    "    return mean_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The cost function is integrated mean squared error between the predicted and true values of the output\n",
    "\n",
    "$$ \\texttt{cost} = \\int d^3 x \\; (\\epsilon ^{NN} _{xc} - \\epsilon ^{\\text{target}} _{xc}) ^ 2$$\n",
    "\n",
    "averaged over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_one_input(params, test_inputs, test_exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, grad = cost_batch(params, test_batch, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The optimization loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optim.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a single update step for the parameters of the model\n",
    "# following optax documentation\n",
    "\n",
    "def update_params(params, batch, opt_state):\n",
    "\n",
    "    inputs, targets = separate_data(batch)\n",
    "    value, grads = cost_batch(params, inputs, targets)\n",
    "\n",
    "    updates, opt_state = optim.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, value, opt_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the optimizer and save loss values for plotting later, into the `losses` array. After this loop is run, the variable `params` will hold optimal parameters and `fxc.apply(params, inputs)` should be identical to evaluating the functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epoch_losses = np.zeros(len(batches))\n",
    "\n",
    "for n in range(100):\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "        params, value, opt_state = update_params(params, batch, opt_state)\n",
    "        epoch_losses[i] = value\n",
    "\n",
    "    loss = np.mean(epoch_losses)\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f\"Epoch: {n+1:3} | Loss: {loss:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names  = [names.split('-')[0].split('/')()(load_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
